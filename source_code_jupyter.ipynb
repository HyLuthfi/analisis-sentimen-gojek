{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Sentimen Ulasan Gojek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 1 Data Acquisition\n",
    "\n",
    "Mengambil ulasan aplikasi Gojek dari Google Play Store menggunakan library `google-play-scraper`. Label sentimen ditentukan berdasarkan rating bintang:\n",
    "- Bintang 4–5 untuk **Positif**\n",
    "- Bintang 3 untuk **Netral**\n",
    "- Bintang 1–2 untuk **Negatif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengambil ulasan: 200/2000\n",
      "Mengambil ulasan: 400/2000\n",
      "Mengambil ulasan: 600/2000\n",
      "Mengambil ulasan: 800/2000\n",
      "Mengambil ulasan: 1000/2000\n",
      "Mengambil ulasan: 1200/2000\n",
      "Mengambil ulasan: 1400/2000\n",
      "Mengambil ulasan: 1600/2000\n",
      "Mengambil ulasan: 1800/2000\n",
      "Mengambil ulasan: 2000/2000\n",
      "\n",
      "Total ulasan berhasil diambil: 2000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google_play_scraper import reviews, Sort\n",
    "import time\n",
    "\n",
    "APP_ID = \"com.gojek.app\"\n",
    "TARGET = 2000\n",
    "OUTPUT = \"data/ulasan_gojek_mentah.csv\"\n",
    "\n",
    "semua_ulasan = []\n",
    "token = None\n",
    "\n",
    "try:\n",
    "    while len(semua_ulasan) < TARGET:\n",
    "        sisa = TARGET - len(semua_ulasan)\n",
    "        batch = min(200, sisa)\n",
    "\n",
    "        hasil, token = reviews(\n",
    "            APP_ID,\n",
    "            lang=\"id\",\n",
    "            country=\"id\",\n",
    "            sort=Sort.NEWEST,\n",
    "            count=batch,\n",
    "            continuation_token=token\n",
    "        )\n",
    "\n",
    "        if not hasil:\n",
    "            break\n",
    "\n",
    "        semua_ulasan.extend(hasil)\n",
    "        print(f\"Mengambil ulasan: {len(semua_ulasan)}/{TARGET}\")\n",
    "\n",
    "        if token is None:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\nTotal ulasan berhasil diambil: {len(semua_ulasan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daftar_data = []\n",
    "for ulasan in semua_ulasan:\n",
    "    daftar_data.append({\n",
    "        \"id_ulasan\": ulasan.get(\"reviewId\", \"\"),\n",
    "        \"nama_pengguna\": ulasan.get(\"userName\", \"\"),\n",
    "        \"isi_ulasan\": ulasan.get(\"content\", \"\"),\n",
    "        \"bintang\": ulasan.get(\"score\", 0),\n",
    "        \"tanggal_ulasan\": ulasan.get(\"at\", \"\"),\n",
    "        \"jumlah_like\": ulasan.get(\"thumbsUpCount\", 0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(daftar_data)\n",
    "\n",
    "def tentukan_sentimen(bintang):\n",
    "    if bintang >= 4:\n",
    "        return \"Positif\"\n",
    "    elif bintang == 3:\n",
    "        return \"Netral\"\n",
    "    else:\n",
    "        return \"Negatif\"\n",
    "\n",
    "df[\"sentimen\"] = df[\"bintang\"].apply(tentukan_sentimen)\n",
    "df = df[df[\"isi_ulasan\"].str.strip() != \"\"]\n",
    "df = df.dropna(subset=[\"isi_ulasan\"])\n",
    "\n",
    "df.to_csv(OUTPUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Data disimpan ke: {OUTPUT}\")\n",
    "print(f\"Total: {len(df)} ulasan\")\n",
    "print(\"\\nDistribusi Sentimen:\")\n",
    "print(df[\"sentimen\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## Tahap 2 Text Cleaning & Pre-processing\n",
    "\n",
    "Pipeline pembersihan teks yang dilakukan:\n",
    "1. Lowercase\n",
    "2. Hapus URL, mention, hashtag\n",
    "3. Hapus emoji dan karakter non-ASCII\n",
    "4. Hapus angka\n",
    "5. Hapus tanda baca\n",
    "6. Normalisasi kata slang/gaul\n",
    "7. Stopword removal (PySastrawi)\n",
    "8. Stemming (PySastrawi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "INPUT  = \"data/ulasan_gojek_mentah.csv\"\n",
    "OUTPUT = \"data/ulasan_gojek_bersih.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT)\n",
    "print(f\"Data dimuat: {len(df)} baris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memuat Stemmer & StopWord Bahasa Indonesia...\")\n",
    "hapus_sw = StopWordRemoverFactory().create_stop_word_remover()\n",
    "stemmer  = StemmerFactory().create_stemmer()\n",
    "\n",
    "kamus_slang = {\n",
    "    \"gk\": \"tidak\", \"ga\": \"tidak\", \"gak\": \"tidak\",\n",
    "    \"ngga\": \"tidak\", \"nggak\": \"tidak\", \"g\": \"tidak\",\n",
    "    \"yg\": \"yang\", \"yng\": \"yang\", \"drpd\": \"daripada\",\n",
    "    \"dgn\": \"dengan\", \"dg\": \"dengan\", \"sy\": \"saya\",\n",
    "    \"sdh\": \"sudah\", \"udh\": \"sudah\", \"udah\": \"sudah\",\n",
    "    \"blm\": \"belum\", \"blom\": \"belum\", \"blum\": \"belum\",\n",
    "    \"dpt\": \"dapat\", \"gmn\": \"bagaimana\", \"gimana\": \"bagaimana\",\n",
    "    \"hrs\": \"harus\", \"krn\": \"karena\", \"karna\": \"karena\",\n",
    "    \"tp\": \"tapi\", \"tpi\": \"tapi\", \"ttg\": \"tentang\",\n",
    "    \"dl\": \"dulu\", \"dlu\": \"dulu\", \"bnyk\": \"banyak\",\n",
    "    \"byk\": \"banyak\", \"msh\": \"masih\", \"masi\": \"masih\",\n",
    "    \"bgt\": \"banget\", \"banget\": \"sangat\", \"aja\": \"saja\",\n",
    "    \"aj\": \"saja\", \"jg\": \"juga\", \"spt\": \"seperti\",\n",
    "    \"ok\": \"oke\", \"lg\": \"lagi\", \"klo\": \"kalau\",\n",
    "    \"kalo\": \"kalau\", \"bs\": \"bisa\", \"bsa\": \"bisa\",\n",
    "    \"dr\": \"dari\", \"utk\": \"untuk\", \"u\": \"untuk\",\n",
    "    \"tdk\": \"tidak\", \"pd\": \"pada\", \"km\": \"kamu\",\n",
    "    \"lo\": \"kamu\", \"lu\": \"kamu\", \"gue\": \"saya\",\n",
    "    \"gw\": \"saya\", \"abis\": \"habis\", \"lbh\": \"lebih\",\n",
    "    \"lbih\": \"lebih\", \"susah\": \"sulit\",\n",
    "    \"aplikasinya\": \"aplikasi\", \"appnya\": \"aplikasi\",\n",
    "}\n",
    "\n",
    "def normalisasi_slang(teks):\n",
    "    return \" \".join([kamus_slang.get(k, k) for k in teks.split()])\n",
    "\n",
    "def bersihkan_teks(teks):\n",
    "    if pd.isna(teks) or str(teks).strip() == \"\":\n",
    "        return \"\"\n",
    "    teks = str(teks).lower()\n",
    "    teks = re.sub(r\"http\\S+|www\\.\\S+\", \"\", teks)\n",
    "    teks = re.sub(r\"@\\w+|#\\w+\", \"\", teks)\n",
    "    teks = teks.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    teks = re.sub(r\"\\d+\", \"\", teks)\n",
    "    teks = re.sub(r\"[^\\w\\s]\", \" \", teks)\n",
    "    teks = re.sub(r\"\\s+\", \" \", teks).strip()\n",
    "    teks = normalisasi_slang(teks)\n",
    "    teks = hapus_sw.remove(teks)\n",
    "    teks = stemmer.stem(teks)\n",
    "    return teks.strip()\n",
    "\n",
    "print(\"Fungsi pre-processing siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memproses teks... (mungkin beberapa menit)\")\n",
    "total = len(df)\n",
    "hasil_bersih = []\n",
    "\n",
    "for i, baris in df.iterrows():\n",
    "    hasil_bersih.append(bersihkan_teks(baris[\"isi_ulasan\"]))\n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"  {i + 1}/{total} selesai...\")\n",
    "\n",
    "df[\"teks_bersih\"] = hasil_bersih\n",
    "df = df[df[\"teks_bersih\"].str.strip() != \"\"]\n",
    "df.to_csv(OUTPUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nSelesai. {len(df)} baris disimpan ke {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tahap 3 Feature Engineering, Modeling & Evaluation\n",
    "\n",
    "- **Feature Engineering:** TF-IDF Vectorizer (5.000 fitur, unigram + bigram)\n",
    "- **Modeling:** Logistic Regression\n",
    "- **Split data:** 80% latih, 20% uji (stratified)\n",
    "- **Evaluasi:** Accuracy, Precision, Recall, F1-Score, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "os.makedirs(\"gambar\", exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(\"data/ulasan_gojek_bersih.csv\")\n",
    "df = df.dropna(subset=[\"teks_bersih\", \"sentimen\"])\n",
    "df = df[df[\"teks_bersih\"].str.strip() != \"\"]\n",
    "\n",
    "distribusi = df[\"sentimen\"].value_counts()\n",
    "print(f\"Data: {len(df)} baris\")\n",
    "print(\"\\nDistribusi Sentimen:\")\n",
    "print(distribusi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"teks_bersih\"]\n",
    "y = df[\"sentimen\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "vektorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = vektorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = vektorizer.transform(X_test)\n",
    "\n",
    "print(f\"Data latih : {len(X_train)} sampel\")\n",
    "print(f\"Data uji   : {len(X_test)} sampel\")\n",
    "print(f\"Jumlah fitur TF-IDF : {X_train_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    solver=\"lbfgs\",\n",
    "    multi_class=\"multinomial\",\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "print(\"Model selesai dilatih.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediksi = model.predict(X_test_tfidf)\n",
    "akurasi  = accuracy_score(y_test, prediksi)\n",
    "\n",
    "print(f\"Akurasi: {akurasi * 100:.2f}%\")\n",
    "print(\"\\nLaporan Klasifikasi:\")\n",
    "print(classification_report(y_test, prediksi, target_names=model.classes_))\n",
    "\n",
    "laporan = classification_report(y_test, prediksi, target_names=model.classes_, output_dict=True)\n",
    "pd.DataFrame(laporan).transpose().to_csv(\"data/laporan_evaluasi.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisasi Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_kelas = model.classes_\n",
    "cm = confusion_matrix(y_test, prediksi, labels=label_kelas)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=label_kelas, yticklabels=label_kelas,\n",
    "            linewidths=0.5, ax=ax)\n",
    "ax.set_title(\"Confusion Matrix — Analisis Sentimen Ulasan Gojek\", fontsize=14, fontweight=\"bold\", pad=15)\n",
    "ax.set_xlabel(\"Prediksi\", fontsize=12)\n",
    "ax.set_ylabel(\"Aktual\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gambar/confusion_matrix.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Confusion matrix disimpan ke: gambar/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisasi Distribusi Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palet = {\"Positif\": \"#2ecc71\", \"Netral\": \"#f39c12\", \"Negatif\": \"#e74c3c\"}\n",
    "warna_bar = [palet.get(k, \"#95a5a6\") for k in distribusi.index]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(\"Distribusi Sentimen Ulasan Gojek\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "batang = ax1.bar(distribusi.index, distribusi.values, color=warna_bar, edgecolor=\"white\", linewidth=1.5)\n",
    "ax1.set_title(\"Jumlah Ulasan per Sentimen\")\n",
    "ax1.set_xlabel(\"Sentimen\")\n",
    "ax1.set_ylabel(\"Jumlah Ulasan\")\n",
    "for p in batang:\n",
    "    ax1.annotate(f\"{p.get_height():,}\",\n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                 ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "ax2.pie(distribusi.values, labels=distribusi.index, colors=warna_bar,\n",
    "        autopct=\"%1.1f%%\", startangle=90,\n",
    "        wedgeprops=dict(edgecolor=\"white\", linewidth=2))\n",
    "ax2.set_title(\"Persentase Sentimen\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gambar/distribusi_sentimen.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Distribusi sentimen disimpan ke: gambar/distribusi_sentimen.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisasi Word Cloud per Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, sumbu = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(\"Word Cloud Ulasan Gojek per Sentimen\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "wc_config = {\n",
    "    \"Positif\": {\"colormap\": \"Greens\",  \"ax\": sumbu[0]},\n",
    "    \"Netral\":  {\"colormap\": \"Oranges\", \"ax\": sumbu[1]},\n",
    "    \"Negatif\": {\"colormap\": \"Reds\",    \"ax\": sumbu[2]},\n",
    "}\n",
    "\n",
    "for label, cfg in wc_config.items():\n",
    "    teks = \" \".join(df[df[\"sentimen\"] == label][\"teks_bersih\"].tolist())\n",
    "    if teks.strip():\n",
    "        wc = WordCloud(width=500, height=300, background_color=\"white\",\n",
    "                       colormap=cfg[\"colormap\"], max_words=80,\n",
    "                       collocations=False).generate(teks)\n",
    "        cfg[\"ax\"].imshow(wc, interpolation=\"bilinear\")\n",
    "    cfg[\"ax\"].set_title(f\"Sentimen: {label}\", fontsize=12, fontweight=\"bold\")\n",
    "    cfg[\"ax\"].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gambar/wordcloud_sentimen.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Word cloud disimpan ke: gambar/wordcloud_sentimen.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpan Model & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model,      \"model/model_sentimen.pkl\")\n",
    "joblib.dump(vektorizer, \"model/vektorizer_tfidf.pkl\")\n",
    "\n",
    "metadata = {\n",
    "    \"akurasi\":        akurasi,\n",
    "    \"jumlah_data\":    len(df),\n",
    "    \"jumlah_latih\":   len(X_train),\n",
    "    \"jumlah_uji\":     len(X_test),\n",
    "    \"jumlah_fitur\":   X_train_tfidf.shape[1],\n",
    "    \"kelas\":          list(model.classes_),\n",
    "    \"distribusi\":     distribusi.to_dict(),\n",
    "    \"presisi_positif\": laporan.get(\"Positif\", {}).get(\"precision\", 0),\n",
    "    \"recall_positif\":  laporan.get(\"Positif\", {}).get(\"recall\", 0),\n",
    "    \"f1_positif\":      laporan.get(\"Positif\", {}).get(\"f1-score\", 0),\n",
    "    \"presisi_negatif\": laporan.get(\"Negatif\", {}).get(\"precision\", 0),\n",
    "    \"recall_negatif\":  laporan.get(\"Negatif\", {}).get(\"recall\", 0),\n",
    "    \"f1_negatif\":      laporan.get(\"Negatif\", {}).get(\"f1-score\", 0),\n",
    "    \"presisi_netral\":  laporan.get(\"Netral\", {}).get(\"precision\", 0),\n",
    "    \"recall_netral\":   laporan.get(\"Netral\", {}).get(\"recall\", 0),\n",
    "    \"f1_netral\":       laporan.get(\"Netral\", {}).get(\"f1-score\", 0),\n",
    "}\n",
    "\n",
    "with open(\"model/metadata_model.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Model  → model/model_sentimen.pkl\")\n",
    "print(\"Vektor → model/vektorizer_tfidf.pkl\")\n",
    "print(\"Meta   → model/metadata_model.json\")\n",
    "print(f\"\\nAkurasi akhir: {akurasi * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
